{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3dad95f",
   "metadata": {},
   "source": [
    "# Exploitation des LLM pour l'extraction d'information\n",
    "\n",
    "- Utilisation d'un LLM depuis une interface python pour la stadardisation des chaines de traitement\n",
    "- Exploitation rapide (sans fine-tuning)\n",
    "\n",
    "<img src=\"ressources/chain.png\"  width=\"600\">\n",
    "\n",
    "On aborde les problèmes séquentiellement en partant du LLM:\n",
    "\n",
    "1. Faire tourner un LLM en python\n",
    "1. Traiter un pdf ou un fichier texte\n",
    "1. Coté LLM, contraindre les sorties et les analyser\n",
    "1. Consuire une chaîne de traitements\n",
    "1. Evaluer les performances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d25b38",
   "metadata": {},
   "source": [
    "# 1. Faire tourner un LLM en python\n",
    "\n",
    "Il existe plusieurs méthodes pour exploiter des LLM dans un programme python:\n",
    "- méthode **locale** (le LLM tourne sur notre ordinateur/serveur)\n",
    "- **API distante** (le LLM tourne chez Google, OpenAI ou autre)\n",
    "\n",
    "## 1.1 Solution avec ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4abbaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clé de l'API google: https://aistudio.google.com/app/u/1/apikey\n",
    "cle_vguigue = # mettre la votre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b9c7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# installation si besoin\n",
    "# ! pip install ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ba0dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "from ollama import chat,generate\n",
    "from ollama import ChatResponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2801347e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the language model\n",
    "ans = ollama.generate(model='gemma3:270m', prompt='Why is the sky blue?')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310dc1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ans.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228d1e0f",
   "metadata": {},
   "source": [
    "### Ne pas confondre la réponse (unique) et le chat (dialogue)\n",
    "\n",
    "Note: generate ne répond qu'à une seule question. Pour passer à une logique de dialogue, il faut utiliser ```chat```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba52c737",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [{\"role\" : \"user\", \"content\": \"Quelle est la différence entre un chat et un chien? Réponse très courte\"}] # on part sur une liste avec des acteurs\n",
    "dialog = ollama.chat(model='llama3.2', messages=messages)\n",
    "\n",
    "print(dialog[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce74d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ajout des éléments de dialogue puis relance\n",
    "messages.append({\"role\": \"assistant\", \"content\": dialog[\"message\"][\"content\"]})\n",
    "messages.append({\"role\": \"user\", \"content\": \"Et lequel est plus indépendant ?\"})\n",
    "\n",
    "dialog = ollama.chat(model='llama3.2', messages=messages)\n",
    "\n",
    "print(dialog[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0ab236",
   "metadata": {},
   "source": [
    "Ne pas hésiter à afficher la liste `messages` pour bien comprendre ce qui se passe dans le dialogue (l'ensemble de la conversation est bien redonnée à chaque fois au modèle de langue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a92f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bac à sable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ee5a3f",
   "metadata": {},
   "source": [
    "### Jouer avec la température\n",
    "\n",
    "Les modèles de langue sont liés à des algorithmes de post-processing, le plus connu étant beam-search: \n",
    "- les sorties sur le prochain mot correspondent à une distribution de probabilité\n",
    "- il est possible de faire des tirages plutôt que de prendre le mot le plus probable\n",
    "- plus la température est faible, plus on prend le mot le plus vraisemblable, plus elle est faible, plus on s'autorise de l'exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f11df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_options = {\"temperature\": 0.}\n",
    "\n",
    "ans = ollama.generate(model='gemma3:270m', prompt='Why is the sky blue?', options=custom_options ) # deterministe\n",
    "print(ans.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685b2ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_options = {\"temperature\": 0.9}\n",
    "ans1 = ollama.generate(model='gemma3:270m', prompt='Why is the sky blue?', options=custom_options ) # stochastique ++\n",
    "ans2 = ollama.generate(model='gemma3:270m', prompt='Why is the sky blue?', options=custom_options ) # \n",
    "print(ans1.response)\n",
    "print(\"---------------------\")\n",
    "print(ans2.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f791ab58",
   "metadata": {},
   "source": [
    "## 1.2 Envisager d'autres API\n",
    "\n",
    "### Google\n",
    "**Il est possible de tester gratuitement les API**\n",
    "\n",
    "Accès aux outils de **google**: [lien](https://aistudio.google.com/app/u/1/apikey?hl=fr&pli=1)\n",
    "- Créer une clé sur la page précédente (+créer un nouveau projet)<BR>\n",
    "Avec les limites suivantes [lien](https://ai.google.dev/gemini-api/docs/rate-limits?hl=fr)\n",
    "\n",
    "### openAI\n",
    "**Il N'est PAS possible de tester gratuitement les API**\n",
    "\n",
    "Guide d'accès: [lien](https://platform.openai.com/docs/overview)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cecd43",
   "metadata": {},
   "source": [
    "#### Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090d73c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pour l'installation\n",
    "# !pip install -q -U google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e320398",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "\n",
    "client = genai.Client(api_key=cle_vguigue)\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    # attention, vous n'avez pas les mêmes droits avec les différents modèles\n",
    "    # gemini-2.5-flash-lite | gemini-2.0-flash | gemini-2.5-pro \n",
    "    model=\"gemini-2.5-flash-lite\", contents=\"Explain how AI works in a few words\"\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fe203a",
   "metadata": {},
   "source": [
    "Il est évidemment possible de régler la température sur ce type de modèle... Et il est possible de jouer de la même manière avec OpenAI, Anthropic ou Perplexity (mais seul Google donne accès à une version de démo gratuite)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1327c44e",
   "metadata": {},
   "source": [
    "## 1.3 Passer par Huggingface\n",
    "\n",
    "ATTENTION: on utilisera Huggingface pour 2 choses assez différentes:\n",
    "1. Des modèles génératifs disponibles pour faire les opérations de ce TP\n",
    "1. Des modèles \"encoders\" pour le TP suivant \n",
    "\n",
    "On peut choisir les modèles parmi la (longue) liste: [lien](https://huggingface.co/models)\n",
    "- Attention à la taille, les ressources deviennent vite conséquantes pour le fonctionnement\n",
    "- privilégier les modèles *à la mode* dans un premier temps (qwen, llama, gemma, phi, ...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cc3e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install transformers, torch\n",
    "# !pip install bitsandbytes\n",
    "### il faut une version ancienne de numpy!\n",
    "# ! pip install numpy==1.26.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a04812b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, numpy\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from transformers import pipeline\n",
    "\n",
    "import re\n",
    "print(transformers.__version__)\n",
    "\n",
    "# Vérifier si le GPU est disponible\n",
    "# pour les PC\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# pour les mac\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0091dd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# on prend un petit modèle pour limiter les calculs\n",
    "model_name = \"Qwen/Qwen3Guard-Gen-0.6B\"\n",
    "\n",
    "# load the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    dtype=\"auto\",\n",
    "    device_map=device,\n",
    "    # quantization_config=quant_config,\n",
    "    torch_dtype=torch.float32, # sur mac, seul le 32 fonctionne, sur PC, on peut descendre à 16\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e906c1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Charger un modèle génératif pré-entraîné \n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer =tokenizer)\n",
    "\n",
    "# Donner un prompt\n",
    "prompt = \"Once upon a time,\"\n",
    "outputs = generator(prompt, max_new_tokens=100)\n",
    "\n",
    "# Afficher le résultat\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332e1a34",
   "metadata": {},
   "source": [
    "## 2. Charger des fichiers de nature différentes (textes, pdf, doc, ...)\n",
    "\n",
    "- Charger un document dans la chaine, que ce soit un texte ou un pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c358ba60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# installation du module\n",
    "# !pip install langchain_community\n",
    "# !pip install docx2txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf69a909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# on passe déjà par langchain... On verra la suite des possibilités un peu plus tard\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader, PyPDFLoader, Docx2txtLoader, UnstructuredFileLoader\n",
    "from langchain_community.document_loaders import UnstructuredFileLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92783088",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path_to_data = \"./ressources/docs\"\n",
    "\n",
    "# Pour charger des fichiers TXT uniquement\n",
    "txt_loader = DirectoryLoader(\n",
    "    path=path_to_data,               # Ton répertoire\n",
    "    glob=\"**/*.txt\",             # Motif pour les fichiers TXT\n",
    "    loader_cls=TextLoader,       # Loader utilisé\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "# Pour charger des fichiers PDF uniquement\n",
    "pdf_loader = DirectoryLoader(\n",
    "    path=path_to_data,\n",
    "    glob=\"**/*.pdf\",\n",
    "    loader_cls=PyPDFLoader,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "docx_loader = DirectoryLoader(\n",
    "    path=path_to_data,\n",
    "    glob=\"**/*.docx\",\n",
    "    loader_cls=Docx2txtLoader\n",
    ")\n",
    "\n",
    "# DOC\n",
    "doc_loader = DirectoryLoader(\n",
    "    path=path_to_data,\n",
    "    glob=\"**/*.doc\",\n",
    "    loader_cls=UnstructuredFileLoader\n",
    ")\n",
    "\n",
    "# Charger les deux types de documents\n",
    "txt_docs = txt_loader.load()\n",
    "pdf_docs = pdf_loader.load()\n",
    "docx_docs = docx_loader.load()\n",
    "doc_docs = doc_loader.load() # note: il n'y en a pas dans le répertoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdb2524",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fusionner les documents\n",
    "all_docs = txt_docs + pdf_docs + docx_docs + doc_docs\n",
    "\n",
    "print(f\"Nombre total de documents chargés : {len(all_docs)}\")\n",
    "print(all_docs[0].page_content[:500])  # aperçu du premier document\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7443ef",
   "metadata": {},
   "source": [
    "N'hésitez pas à jouer avec all_docs et à faire le lien avec les documents effectivement présent dans le répertoire `ressources/docs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d47ba22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pour vos tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808da784",
   "metadata": {},
   "source": [
    "# 3. Analyser un texte avec un LLM\n",
    "\n",
    "On va travailler avec ollama et HuggingFace pour tenter d'analyser un texte\n",
    "\n",
    "1. Construction d'un prompt\n",
    "1. Passage du prompt et du texte à analyser au LLM\n",
    "1. Optimiser / jouer avec le prompt\n",
    "1. Contrainte sur la réponse\n",
    "\n",
    "Note: on va construire la chaine avec un petit modèle de langue, pour ne pas perdre inutilement de temps et d'énergie... Ce modèle n'est pas le plus performant!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee22478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copie de la cellule initiale = chargement de ollama\n",
    "\n",
    "# import ollama\n",
    "# from ollama import chat,generate\n",
    "# from ollama import ChatResponse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3bd3a4",
   "metadata": {},
   "source": [
    "## 3.1 Construction du prompt\n",
    "\n",
    "Demander au modèle de langue de récupérer les personnes, les lieux, les organisations et les dates dans un texte\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c9f907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question préliminaire:\n",
    "# commençons par visualiser le premier texte du corpus\n",
    "txt = all_docs[0].page_content\n",
    "print(txt)\n",
    "# il est possible de récupérer d'autres informations que le contenu (e.g. le nom de fichier)\n",
    "source = all_docs[0].metadata['source']\n",
    "print(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fc0dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt naïf\n",
    "prompt = \"Trouver les personnes, les lieux, les organisations et les dates dans le texte: \"\n",
    "\n",
    "to_analyse = prompt + txt\n",
    "\n",
    "ans = ollama.generate(model='gemma3:270m', prompt=to_analyse)\n",
    "print(ans.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3d53e9",
   "metadata": {},
   "source": [
    "## 3.2 Optimisation du prompt\n",
    "\n",
    "Proposition de construction interactive: on va utiliser chatgpt (ou le modèle de votre choix) pour tenter de construire un prompt efficace. Je vous propose le texte suivant (modifiable à souhait):\n",
    "\n",
    "```\n",
    "Je veux construire un prompt pour analyser des textes. Mon but est d'extraire les personnes, les lieux, les organisations et les dates. Peux tu me faire une proposition?\n",
    "```\n",
    "\n",
    "Note: dans mes essais, le chatbot anticipe et propose dès le début de formatter les réponses...\n",
    "Note 2: la proposition de la boite ci-dessous correspond à la réponse de chatgpt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d13d7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construction d'un prompt \n",
    "prompt = \"\"\"\n",
    "Rôle : Tu es un système d’analyse de texte spécialisé dans l’extraction d’entités nommées.\n",
    "Tâche :\n",
    "Analyse le texte suivant et identifie les entités mentionnées. Classe-les dans les catégories suivantes :\n",
    "Personnes : noms de personnes individuelles.\n",
    "Lieux : pays, villes, régions, adresses, lieux géographiques.\n",
    "Organisations : entreprises, institutions, associations, administrations, etc.\n",
    "Dates : jours précis, mois, années, périodes temporelles.\n",
    "\n",
    "Format de sortie attendu (JSON clair et structuré) :\n",
    "{\n",
    "  \"Personnes\": [\"...\"],\n",
    "  \"Lieux\": [\"...\"],\n",
    "  \"Organisations\": [\"...\"],\n",
    "  \"Dates\": [\"...\"]\n",
    "}\n",
    "\n",
    "Texte à analyser :\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1409f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "to_analyse = prompt + txt\n",
    "\n",
    "ans = ollama.generate(model='gemma3:270m', prompt=to_analyse)\n",
    "print(ans.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c5d87e",
   "metadata": {},
   "source": [
    "## 3.3 Passage à un modèle plus performant (via ollama ou les API google)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e74d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = ollama.generate(model='llama3.2', prompt=to_analyse)\n",
    "print(ans.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e141085",
   "metadata": {},
   "source": [
    "Impact du prompt: seuls les noms propres sont considérés comme des personnes. \n",
    "\n",
    "Si on modifie:\n",
    "`Personnes : noms de personnes individuelles.`<BR>\n",
    "en: `Personnes : mentions, nom, désignation de personnes`\n",
    "\n",
    "Pour améliorer les performances, il est possible de donner des exemples pour les différentes catégories... Vous pouvez les inventer ou en demander à votre chatbot favori.\n",
    "\n",
    "Comment forcer le LLM à ne sortir qu'un JSON mais pas d'explications? <BR>\n",
    "Note: n'hésitez pas à demander à un chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3409a732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : jouer avec le prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a03ac1",
   "metadata": {},
   "source": [
    "### Avec une API Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d03bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "\n",
    "client = genai.Client(api_key=cle_vguigue)\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    # Gemini 2.5 Flash-Lite | Gemini 2.0 Flash | Gemini 2.5 Pro\n",
    "    model=\"gemini-2.5-flash-lite\", contents=to_analyse\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3b5e92",
   "metadata": {},
   "source": [
    "# 4. Construire une chaîne de traitement\n",
    "\n",
    "Introduction de `langchain` (déjà croisé pour l'importation des documents) pour la construction d'une chaine générique où les outils sont interchangeable\n",
    "\n",
    "1. Charge tous les documents d'un répertoire (en txt ou pdf) \n",
    "2. Passe chaque document dans un llm avec un prompt fixé pour obtenir un json \n",
    "3. Vérifie que le JSON est bien formaté\n",
    "4. Changer de modèle de langue pour montrer l'intérêt de la chaîne\n",
    "\n",
    "On va créer une fonction pour chaque étape et construire la chaîne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacb9f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install langchain_ollama "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadd07e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from langchain_community.document_loaders import TextLoader, PyPDFLoader\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.runnables.base import RunnableSequence\n",
    "from langchain.schema import BaseOutputParser\n",
    "from langchain_ollama.llms import OllamaLLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408c316f",
   "metadata": {},
   "source": [
    "## 4.1 Construction d'une chaîne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a3169c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------- 1. Charger tous les documents --------\n",
    "# l'approche est plus élégante que tout à l'heure: on s'interroge sur les types de fichier au fur et à mesure\n",
    "\n",
    "def load_documents_from_directory(directory_path):\n",
    "    docs = []\n",
    "    for filename in os.listdir(directory_path):\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "        if filename.endswith(\".txt\"):\n",
    "            loader = TextLoader(file_path, encoding=\"utf-8\")\n",
    "            docs.extend(loader.load())\n",
    "        elif filename.endswith(\".pdf\"):\n",
    "            loader = PyPDFLoader(file_path)\n",
    "            docs.extend(loader.load())\n",
    "    return docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186af92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- 2. Définir le LLM + Prompt --------\n",
    "\n",
    "# Construction d'un prompt [copie de la cellule définie dans la section précédente]\n",
    "# ATTENTION => Ajout du document dans le prompt + doublement des autres accolades\n",
    "\n",
    "prompt = \"\"\"\n",
    "Rôle : Tu es un système d’analyse de texte spécialisé dans l’extraction d’entités nommées.\n",
    "Tâche :\n",
    "Analyse le texte suivant et identifie les entités mentionnées. Classe-les dans les catégories suivantes :\n",
    "Personnes : noms de personnes individuelles.\n",
    "Lieux : pays, villes, régions, adresses, lieux géographiques.\n",
    "Organisations : entreprises, institutions, associations, administrations, etc.\n",
    "Dates : jours précis, mois, années, périodes temporelles.\n",
    "\n",
    "Format de sortie attendu (JSON clair et structuré) :\n",
    "{{\n",
    "  \"Personnes\": [\"...\"],\n",
    "  \"Lieux\": [\"...\"],\n",
    "  \"Organisations\": [\"...\"],\n",
    "  \"Dates\": [\"...\"]\n",
    "}}\n",
    "\n",
    "Texte à analyser :\n",
    "{document}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5dcd97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM et prompt\n",
    "# Note: le LLM est dans un objet Langchain (astuce pour les rendre interchangeable => cf question suivante)\n",
    "\n",
    "llm = OllamaLLM(model=\"llama3.2\")\n",
    "request = PromptTemplate.from_template(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344924cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------- 3. Validation et récupération du JSON --------\n",
    "# Parser JSON robuste\n",
    "class SafeJsonOutputParser(BaseOutputParser):\n",
    "    def parse(self, text: str):\n",
    "        try:\n",
    "            return json.loads(text)\n",
    "        except json.JSONDecodeError:\n",
    "            # print(\"ERR: \", text)\n",
    "            # tentative de réparation si l'output n'est pas un JSON strict\n",
    "            match = re.search(r\"\\{(.*)\\}\", text, re.DOTALL)\n",
    "            text = \"{\"+match.group(1)+\"}\" # se limiter à ce qui se trouve dans les accolades\n",
    "            return json.loads(text)\n",
    "\n",
    "json_parser = SafeJsonOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b44b11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- 4. Pipeline complet --------\n",
    "\n",
    "chain = request | llm | json_parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3ba30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_documents(directory_path):\n",
    "    docs = load_documents_from_directory(directory_path)\n",
    "    results = dict()\n",
    "    for doc in docs:\n",
    "        print(doc.metadata)\n",
    "        try:\n",
    "            res = chain.invoke({\"document\": doc.page_content})\n",
    "            results[doc.metadata['source']] = res\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur avec {doc.metadata}: {e}\")\n",
    "    return results\n",
    "    \n",
    "# calcul sur l'ensemble des documents\n",
    "output = process_documents(\"./ressources/docs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5050e8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vérification des sorties\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4cf3e8",
   "metadata": {},
   "source": [
    "## 4.2 Changer de modèle de langue, en conservant la chaîne\n",
    "\n",
    "Passage à gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c01871d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install langchain-google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e542f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Crée un LLM basé sur Gemini\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-lite\", api_key=cle_vguigue)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163bb36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE INCHANGE\n",
    "# -------- 4. Pipeline complet --------\n",
    "\n",
    "chain = request | llm | json_parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d319ecce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE INCHANGE\n",
    "\n",
    "def process_documents(directory_path):\n",
    "    docs = load_documents_from_directory(directory_path)\n",
    "    results = dict()\n",
    "    for doc in docs:\n",
    "        print(doc.metadata)\n",
    "        try:\n",
    "            res = chain.invoke({\"document\": doc.page_content})\n",
    "            results[doc.metadata['source']] = res\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur avec {doc.metadata}: {e}\")\n",
    "    return results\n",
    "    \n",
    "# calcul sur l'ensemble des documents\n",
    "output = process_documents(\"./ressources/docs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4148d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418e2534",
   "metadata": {},
   "source": [
    "# 5. Evaluation des performances\n",
    "\n",
    "On dispose d'une vérité terrain (toute relative) dans le fichier `ressources/verite_terrain.pkl`. Nous allons le charger et comparer les contenus, ce qui n'a rien de trivial!\n",
    "\n",
    "1. Comparer seulement les résultats pour lesquels nous avons réussi l'extraction\n",
    "1. Comparer les listes par types\n",
    "1. [OPT] Introduire une distance seuil pour savoir si les entités sont les mêmes ou pas (e.g. la Manche *vs* Manche)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e8ab95",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af37f28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Récupération du fichier\n",
    "\n",
    "import pickle as pkl\n",
    "\n",
    "with open(\"ressources/verite_terrain.pkl\", \"rb\") as f:\n",
    "    gt = pkl.load(f)\n",
    "\n",
    "# le formatage du fichier est le suivant (et le même que celui de la chaine ci-dessus):\n",
    "# gt[nom_du_fichier] = {\"Personnes\":[...], \"Lieux\": ...}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fcbe45",
   "metadata": {},
   "source": [
    "## 5.1 Version basique en \"exact match\"\n",
    "\n",
    "Dans un premier temps, on calcule le pourcentage de matching par classe.\n",
    "\n",
    "Il faudrait ensuite  calculer pour chaque classe la précision et le rappel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32593b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "res = None\n",
    "\n",
    "# parcours de tous les documents\n",
    "for k in gt.keys():\n",
    "    y = gt[k]\n",
    "\n",
    "    if res == None:\n",
    "        res = dict()\n",
    "        for ke in y:\n",
    "            res[ke] = 0\n",
    "            res[ke+\"-tot\"] = 0\n",
    "\n",
    "    \n",
    "    try:\n",
    "        yhat = output['./'+k]\n",
    "        # parcours des types d'entites\n",
    "        for ke in y.keys():\n",
    "            # TP\n",
    "            # print(y[ke], yhat[ke])\n",
    "            nb_commun = np.intersect1d(y[ke], yhat[ke])\n",
    "            res[ke] += len(nb_commun)\n",
    "            res[ke+\"-tot\"] += len(y[ke]) \n",
    "    except Exception as e:\n",
    "        # print(k)\n",
    "        for ke in y:\n",
    "            res[ke+\"-tot\"] += len(y[ke]) \n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e368df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pourcentage de reconnaissance par classe:\n",
    "\n",
    "for k in res.keys():\n",
    "    if \"-tot\" in k: continue\n",
    "    print(k, res[k]/res[k+'-tot'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef2bc40",
   "metadata": {},
   "source": [
    "## 5.2 Vers une version plus robuste\n",
    "\n",
    "Introduction de la distance d'édition\n",
    "1. Comprendre la distance de Levenstein\n",
    "1. La Comparaison devient bien plus difficile: il faut mesurer toutes les distances et seuiller!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85fe182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5c3de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import Levenshtein\n",
    "\n",
    "mot1 = \"chat\"\n",
    "mot2 = \"chats\"\n",
    "mot3 = \"chien\"\n",
    "\n",
    "# Calcul de la distance d'édition\n",
    "dist1 = Levenshtein.distance(mot1, mot2)\n",
    "dist2 = Levenshtein.distance(mot1, mot3)\n",
    "\n",
    "print(f\"Distance entre '{mot1}' et '{mot2}' : {dist1}\")\n",
    "print(f\"Distance entre '{mot1}' et '{mot3}' : {dist2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172321ff",
   "metadata": {},
   "source": [
    "# 6. Pour aller plus loin\n",
    "\n",
    "Adapter la chaine de traitement pour extraire les entités mais aussi les relations entre ces entités\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7953d77",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyth-torch-numpy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
